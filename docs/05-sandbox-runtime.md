# 5. The Sandbox Runtime

This document provides a deep dive into `entrypoint.py`â€”the generated Python script that runs inside the container and provides the execution environment for LLM-generated code.

## ğŸ¯ Purpose

The entrypoint script is the "operating system" for the sandbox. It provides:

| Component | Purpose |
|-----------|---------|
| **Stream Proxies** | Redirect print() to JSON messages |
| **Stdin Reader** | Receive commands and RPC responses from host |
| **RPC Mechanism** | Send requests to host, wait for responses |
| **MCP Modules** | Fake `mcp.runtime` and `mcp.servers` packages |
| **MCP Proxies** | `mcp_weather`, `mcp_soccer`, etc. |
| **Runtime Helpers** | `save_memory()`, `save_tool()`, etc. |
| **Main Loop** | Wait for code, execute, repeat |

---

## ğŸ“œ Generated Code Structure

The entrypoint is generated by `_render_entrypoint()` and looks like this:

```python
# ==========================================
# SECTION 1: IMPORTS
# ==========================================
import asyncio
import inspect
import json
import sys
import traceback
import types
from contextlib import suppress
from pathlib import Path

# ==========================================
# SECTION 2: CONFIGURATION
# ==========================================
AVAILABLE_SERVERS = [
    {
        "name": "weather",
        "alias": "weather",
        "tools": [
            {"name": "get_weather", "alias": "get_weather", ...},
            {"name": "get_forecast", "alias": "get_forecast", ...}
        ]
    }
]

DISCOVERED_SERVERS = {
    "weather": "Get weather information for cities",
    "soccer": "Get live soccer matches and standings"
}

# Paths for persistence
USER_TOOLS_PATH = Path("/projects/user_tools.py")
MEMORY_DIR = Path("/projects/memory")

# RPC tracking
_PENDING_RESPONSES = {}
_REQUEST_COUNTER = 0
_EXECUTION_QUEUE = asyncio.Queue()
```

---

## ğŸ”„ Stream Proxies

The sandbox can't just use regular `print()` because we need to capture output as structured JSON:

```python
# ==========================================
# SECTION 3: STREAM PROXIES
# ==========================================

def _send_message(message):
    """Send a JSON message to the host."""
    sys.__stdout__.write(json.dumps(message, separators=(",", ":")) + "\n")
    sys.__stdout__.flush()


class _StreamProxy:
    """Intercept writes and convert to JSON messages."""
    
    def __init__(self, kind):
        self._kind = kind  # "stdout" or "stderr"
    
    def write(self, data):
        if not data:
            return
        # Instead of writing to real stdout, send a JSON message
        _send_message({"type": self._kind, "data": data})
    
    def flush(self):
        pass
    
    def isatty(self):
        return False


# Replace sys.stdout and sys.stderr with proxies
sys.stdout = _StreamProxy("stdout")
sys.stderr = _StreamProxy("stderr")
```

**Before proxying:**
```python
print("Hello")
# Output: Hello\n  (raw text)
```

**After proxying:**
```python
print("Hello")
# Output: {"type":"stdout","data":"Hello\n"}  (JSON)
```

Note: We use `sys.__stdout__` (with double underscores) to access the *real* stdout for sending JSON messages to the host. `sys.stdout` (single underscore) is our proxy.

---

## ğŸ“¨ Stdin Reader

The container needs to receive two types of messages:
1. `execute` - New code to run
2. `rpc_response` - Responses to our RPC calls

```python
# ==========================================
# SECTION 4: STDIN READER
# ==========================================

async def _stdin_reader():
    """Background task that reads messages from stdin."""
    
    # Set up async stdin reading
    loop = asyncio.get_running_loop()
    reader = asyncio.StreamReader()
    protocol = asyncio.StreamReaderProtocol(reader)
    await loop.connect_read_pipe(lambda: protocol, sys.stdin)
    
    while True:
        # Read one line (JSON message)
        line = await reader.readline()
        if not line:
            sys.exit(0)  # Host closed stdin
        
        try:
            message = json.loads(line.decode())
        except Exception:
            continue
        
        msg_type = message.get("type")
        
        if msg_type == "rpc_response":
            # This is a response to an RPC we sent
            request_id = message.get("id")
            future = _PENDING_RESPONSES.pop(request_id, None)
            if future and not future.done():
                if message.get("success", True):
                    future.set_result(message.get("payload"))
                else:
                    future.set_exception(RuntimeError(message.get("error")))
        
        elif msg_type == "execute":
            # New code to run - add to queue
            await _EXECUTION_QUEUE.put(message.get("code"))
```

---

## ğŸ“ RPC Mechanism

When code in the sandbox needs to call an MCP tool, it can't do so directly (no network). Instead, it sends an RPC request to the host:

```python
# ==========================================
# SECTION 5: RPC MECHANISM
# ==========================================

async def _rpc_call(payload):
    """Send an RPC request to the host and wait for response."""
    
    loop = asyncio.get_running_loop()
    
    # Generate unique request ID
    global _REQUEST_COUNTER
    _REQUEST_COUNTER += 1
    request_id = _REQUEST_COUNTER
    
    # Create a Future to wait on
    future = loop.create_future()
    _PENDING_RESPONSES[request_id] = future
    
    # Send request to host
    _send_message({
        "type": "rpc_request",
        "id": request_id,
        "payload": payload
    })
    
    # Wait for response (blocking)
    return await future
```

**How it works:**

```
Container                                      Host
    â”‚                                            â”‚
    â”‚  â”€â”€â”€â”€ RPC Request (id=1) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º    â”‚
    â”‚       {"type": "rpc_request",              â”‚
    â”‚        "id": 1,                            â”‚
    â”‚        "payload": {...}}                   â”‚
    â”‚                                            â”‚
    â”‚  (Container waits on future)               â”‚
    â”‚                                            â”‚
    â”‚  â—„â”€â”€â”€â”€ RPC Response (id=1) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚
    â”‚        {"type": "rpc_response",            â”‚
    â”‚         "id": 1,                           â”‚
    â”‚         "payload": {...}}                  â”‚
    â”‚                                            â”‚
    â”‚  (Future resolves, code continues)         â”‚
    â”‚                                            â”‚
```

---

## ğŸ“¦ MCP Modules

The sandbox creates fake Python packages so code can do `import mcp.runtime`:

```python
# ==========================================
# SECTION 6: MCP MODULES
# ==========================================

def _install_mcp_modules():
    """Create mcp, mcp.runtime, and mcp.servers modules."""
    
    # Create mcp package
    mcp_package = types.ModuleType("mcp")
    mcp_package.__path__ = []
    sys.modules["mcp"] = mcp_package
    
    # Create mcp.runtime module
    runtime_module = types.ModuleType("mcp.runtime")
    mcp_package.runtime = runtime_module
    sys.modules["mcp.runtime"] = runtime_module
    
    # Create mcp.servers module
    servers_module = types.ModuleType("mcp.servers")
    mcp_package.servers = servers_module
    sys.modules["mcp.servers"] = servers_module
    
    # Add helper functions to runtime module
    runtime_module.discovered_servers = discovered_servers
    runtime_module.list_tools = list_tools
    runtime_module.query_tool_docs = query_tool_docs
    runtime_module.save_memory = save_memory
    runtime_module.load_memory = load_memory
    # ... more helpers ...
    
    return runtime_module
```

Now LLM code can do:
```python
import mcp.runtime as runtime
servers = runtime.discovered_servers()
```

---

## ğŸ› ï¸ Runtime Helpers

The runtime module provides various helper functions:

### Discovery Helpers

```python
def discovered_servers(detailed=False):
    """List all available MCP servers."""
    if detailed:
        return tuple({"name": k, "description": v} for k, v in DISCOVERED_SERVERS.items())
    return tuple(DISCOVERED_SERVERS.keys())
    # â†’ ('weather', 'soccer')

async def list_tools(server):
    """Get tools for a specific server (via RPC)."""
    response = await _rpc_call({"type": "list_tools", "server": server})
    return response.get("tools", [])

async def query_tool_docs(server, tool=None, detail="summary"):
    """Get documentation for server tools."""
    # ... RPC call ...

async def search_tool_docs(query, *, limit=5, detail="summary"):
    """Search for tools matching a query."""
    # ... RPC call ...
```

### Persistence Helpers

```python
def save_memory(key, value, *, metadata=None):
    """Save data to persistent memory."""
    sanitized_key = _sanitize_memory_key(key)
    memory_file = MEMORY_DIR / f"{sanitized_key}.json"
    
    MEMORY_DIR.mkdir(parents=True, exist_ok=True)
    
    data = {
        "key": key,
        "value": value,
        "metadata": metadata or {},
        "created_at": datetime.now().isoformat(),
        "updated_at": datetime.now().isoformat(),
    }
    memory_file.write_text(json.dumps(data, indent=2))
    return f"Memory '{key}' saved."

def load_memory(key, *, default=None):
    """Load data from persistent memory."""
    sanitized_key = _sanitize_memory_key(key)
    memory_file = MEMORY_DIR / f"{sanitized_key}.json"
    
    if not memory_file.exists():
        return default
    
    data = json.loads(memory_file.read_text())
    return data.get("value", default)

def list_memories():
    """List all saved memory keys."""
    # Returns list of {key, metadata, created_at, updated_at}

def update_memory(key, updater):
    """Update a memory value using a function."""
    current = load_memory(key)
    new_value = updater(current)
    save_memory(key, new_value)
    return new_value
```

### Tool Persistence

```python
def save_tool(func):
    """Save a function for future sessions."""
    source = inspect.getsource(func)
    USER_TOOLS_PATH.parent.mkdir(parents=True, exist_ok=True)
    
    # Append to user_tools.py
    with open(USER_TOOLS_PATH, "a") as f:
        f.write(f"\n\n{source}")
    
    return f"Tool '{func.__name__}' saved."
```

---

## ğŸ­ MCP Proxy Class

The magic that makes `mcp_weather.get_weather()` work:

```python
# ==========================================
# SECTION 7: MCP PROXY
# ==========================================

class _MCPProxy:
    """Proxy object that converts method calls to RPC requests."""
    
    def __init__(self, server_info):
        self._server_name = server_info["name"]
        self._tools = {tool["alias"]: tool for tool in server_info.get("tools", [])}
    
    async def list_tools(self):
        """Get tools from this server."""
        response = await _rpc_call({
            "type": "list_tools",
            "server": self._server_name,
        })
        return response.get("tools", [])
    
    def __getattr__(self, tool_alias):
        """Magic method - called when accessing any attribute."""
        
        # Look up the real tool name
        tool = self._tools.get(tool_alias)
        target = tool.get("name") if tool else tool_alias
        
        # Return an async function that does the RPC
        async def _invoke(_target=target, **kwargs):
            response = await _rpc_call({
                "type": "call_tool",
                "server": self._server_name,
                "tool": _target,
                "arguments": kwargs,
            })
            if not response.get("success", True):
                raise RuntimeError(response.get("error", "MCP call failed"))
            return response.get("result")
        
        return _invoke
```

**How `__getattr__` works:**

```python
# When you write:
await mcp_weather.get_weather(city="Seattle")

# Python does:
# 1. Look up 'get_weather' on mcp_weather object
# 2. It's not a real attribute, so call __getattr__("get_weather")
# 3. __getattr__ returns an async function
# 4. You call that function with city="Seattle"
# 5. The function sends RPC and returns result
```

---

## ğŸŒ Global Namespace Setup

All the proxies are injected into the global namespace where LLM code runs:

```python
# ==========================================
# SECTION 8: GLOBAL NAMESPACE
# ==========================================

# Install MCP modules
runtime_module = _install_mcp_modules()

# Create global namespace for user code
_GLOBAL_NAMESPACE = {"__name__": "__sandbox__"}

# Add mcp package
_GLOBAL_NAMESPACE["mcp"] = __import__("mcp")

# Track loaded servers
LOADED_MCP_SERVERS = tuple(server["name"] for server in AVAILABLE_SERVERS)
mcp_servers = {}

# Create proxy for each server and inject into namespace
for server in AVAILABLE_SERVERS:
    proxy = _MCPProxy(server)
    mcp_servers[server["name"]] = proxy
    _GLOBAL_NAMESPACE[f"mcp_{server['alias']}"] = proxy  # mcp_weather

_GLOBAL_NAMESPACE["mcp_servers"] = mcp_servers
_GLOBAL_NAMESPACE["LOADED_MCP_SERVERS"] = LOADED_MCP_SERVERS

# Also inject helpers directly
_GLOBAL_NAMESPACE["save_memory"] = save_memory
_GLOBAL_NAMESPACE["load_memory"] = load_memory
_GLOBAL_NAMESPACE["save_tool"] = save_tool
# ...
```

Now when LLM code executes, it can access:
- `mcp_weather` - Proxy for weather server
- `mcp_soccer` - Proxy for soccer server
- `save_memory()` - Direct access to helper
- `mcp.runtime` - Can import the module

---

## âš¡ Code Execution

How the LLM's code actually runs:

```python
# ==========================================
# SECTION 9: CODE EXECUTION
# ==========================================

async def _execute_code(code):
    """Execute user-provided Python code."""
    try:
        # Compile with support for top-level await
        # PyCF_ALLOW_TOP_LEVEL_AWAIT lets you use 'await' without 'async def'
        flags = getattr(__import__("ast"), "PyCF_ALLOW_TOP_LEVEL_AWAIT", 0)
        compiled = compile(code, "<sandbox>", "exec", flags=flags)
        
        # Execute in our prepared namespace
        result = eval(compiled, _GLOBAL_NAMESPACE, _GLOBAL_NAMESPACE)
        
        # If result is a coroutine, await it
        if inspect.isawaitable(result):
            await result
            
    except SystemExit:
        raise  # Let sys.exit() work
    except BaseException:
        traceback.print_exc()  # Print errors to stderr proxy
```

**Key features:**

1. **Top-level await**: LLM can write `await foo()` without wrapping in `async def`
2. **Persistent namespace**: Variables survive between executions
3. **Error handling**: Exceptions are printed, not crashed

---

## ğŸ” Main Loop

The entrypoint runs forever, waiting for code:

```python
# ==========================================
# SECTION 10: MAIN LOOP
# ==========================================

async def _main_loop():
    """Main execution loop."""
    
    # Start the stdin reader as a background task
    asyncio.create_task(_stdin_reader())
    
    # Forever loop
    while True:
        # Wait for code from the queue
        code = await _EXECUTION_QUEUE.get()
        
        # Execute it
        await _execute_code(code)
        
        # Tell host we're done
        _send_message({"type": "execution_done"})


if __name__ == "__main__":
    try:
        asyncio.run(_main_loop())
    except KeyboardInterrupt:
        pass
```

**Timeline:**

```
Container starts
      â”‚
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ _main_loop()    â”‚
â”‚                 â”‚
â”‚ Start stdin     â”‚
â”‚ reader task     â”‚
â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Wait for code...â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 â”‚                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                        â”‚
         â”‚ (Host sends {"type": "execute", ...})           â”‚
         â–¼                                                 â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                        â”‚
â”‚ Execute code    â”‚                                        â”‚
â”‚                 â”‚                                        â”‚
â”‚ (May send RPC   â”‚                                        â”‚
â”‚  requests)      â”‚                                        â”‚
â”‚                 â”‚                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                        â”‚
         â”‚                                                 â”‚
         â–¼                                                 â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                        â”‚
â”‚ Send            â”‚                                        â”‚
â”‚ execution_done  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚                 â”‚        (Loop back)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ§  Why This Design?

### Why generate code instead of sending it dynamically?

We could theoretically send all the infrastructure code over stdin each time. But:

1. **Performance**: Writing a file once is faster than sending 600 lines per execution
2. **Reliability**: File-based approach is more robust
3. **Debugging**: You can inspect `/ipc/entrypoint.py` if something goes wrong

### Why use `__getattr__` magic?

It creates a clean API without knowing tool names ahead of time:

```python
# With __getattr__, any method call works:
await mcp_weather.get_weather(...)
await mcp_weather.get_forecast(...)
await mcp_weather.any_new_tool_added_later(...)
```

### Why async throughout?

MCP tool calls can be slow (network requests). Async allows:
- Multiple RPC calls in parallel
- Non-blocking wait for responses
- Better resource utilization

---

## Next Steps

â†’ [MCP Proxies & RPC](06-mcp-proxies-and-rpc.md) - More detail on communication
